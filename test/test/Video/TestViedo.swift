//
//  TestViedo.swift
//  test
//
//  Created by cyanboo on 2022/7/13.
//

import Foundation
import UIKit
import AVFoundation
import Photos
import Vision
import AVKit

class TestViedo: UIViewController, AVCaptureFileOutputRecordingDelegate {
    
    ///å½•åƒ
    // å½•åƒæŒ‰é’®
    var recordButton: UIButton!
    // æ­£åœ¨å½•éŸ³
    var isRecording = false
    //æœ€å¤§å…è®¸çš„å½•åˆ¶æ—¶é—´ï¼ˆç§’ï¼‰
    let totalSeconds: Float64 = 10.00
    //æ¯ç§’å¸§æ•°
    var framesPerSecond:Int32 = 15
    //ä¿å­˜æ‰€æœ‰çš„å½•åƒç‰‡æ®µæ•°ç»„
    var videoAssets = [AVAsset]()
    //è§†é¢‘æ•è·ä¼šè¯ã€‚å®ƒæ˜¯inputå’Œoutputçš„æ¡¥æ¢ã€‚å®ƒåè°ƒç€intputåˆ°outputçš„æ•°æ®ä¼ è¾“
    let captureSession = AVCaptureSession()
    //è§†é¢‘è¾“å…¥è®¾å¤‡
    let videoDevice = AVCaptureDevice.default(for: AVMediaType.video)
    //    let movieDevice = AVCaptureDevice.default(for: .audio
    //éŸ³é¢‘è¾“å…¥è®¾å¤‡
    let audioDevice = AVCaptureDevice.default(for: AVMediaType.audio)
    //å°†æ•è·åˆ°çš„è§†é¢‘è¾“å‡ºåˆ°æ–‡ä»¶
    let fileOutput = AVCaptureMovieFileOutput()
    
    ///è„¸éƒ¨è¯†åˆ«
    let session = AVCaptureSession()
    let dataOutputQueue = DispatchQueue(
        label: "video data queue",
        qos: .userInitiated,
        attributes: [],
        autoreleaseFrequency: .workItem)
    var faceViewHidden = false
    var maxX: CGFloat = 0.0
    var midY: CGFloat = 0.0
    var maxY: CGFloat = 0.0
    var sequenceHandler = VNSequenceRequestHandler()
    var videoOutputFace = AVCaptureVideoDataOutput()
    
    
    override func viewDidLoad() {
        super.viewDidLoad()
                setupFace()
        //        otherVC()
        // è®¾ç½®é¢„è§ˆç”»é¢
        setUpPreview()
        session.startRunning()
    }
    
    
    @objc func onClickRecordButton(sender: UIButton) {
        session.stopRunning()
        if !isRecording {
            printLog(isRecording)
            // éŒ²ç”»é–‹å§‹
            let paths = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true)
            let documentsDirectory = paths[0] as String
            let filePath : String? = "\(documentsDirectory)/temp.mp4"
            let fileURL :NSURL = NSURL(fileURLWithPath: filePath!)
            
            fileOutput.startRecording(to: fileURL as URL, recordingDelegate:self)
            isRecording=true
            changeButtonColor(target:recordButton, color:UIColor.red)
            recordButton.setTitle("éŒ²ç”»ä¸­", for: .normal)
            self.captureSession.startRunning()

        } else {
            printLog(isRecording)
            // éŒ²ç”»çµ‚äº†
            fileOutput.stopRecording()
            isRecording=false
            changeButtonColor(target:recordButton, color:UIColor.gray)
            recordButton.setTitle("éŒ²ç”»é–‹å§‹", for: .normal)
        }
    }
    
    func changeButtonColor(target:UIButton, color:UIColor) {
        target.backgroundColor = color
    }
    
    //å½•åƒå¼€å§‹çš„ä»£ç†æ–¹æ³•
    func fileOutput(_ output: AVCaptureFileOutput,
                    didStartRecordingTo fileURL: URL,
                    from connections: [AVCaptureConnection]) {
        printLog(output)
    }
    
    func fileOutput(_ output:AVCaptureFileOutput, didFinishRecordingTo outputFileURL:URL, from connections: [AVCaptureConnection], error:Error?) {
        printLog(output)
        //        // ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¸ä¿å­˜
        //        PHPhotoLibrary.shared().performChanges({
        //            PHAssetChangeRequest.creationRequestForAssetFromVideo(atFileURL: outputFileURL)
        //        }) { completed, error in
        //            if completed {
        //                print("Video is saved!")
        //            }
        //        }
        var message:String!
        //å°†å½•åˆ¶å¥½çš„å½•åƒä¿å­˜åˆ°ç…§ç‰‡åº“ä¸­
        PHPhotoLibrary.shared().performChanges({
            PHAssetChangeRequest.creationRequestForAssetFromVideo(atFileURL: outputFileURL)
            //            Data(contentsOfurl)
            //            let data = NSData(contentsOf: outputFileURL)
            //            print(data)
            printLog(outputFileURL)
            
        }, completionHandler: { (isSuccess: Bool, error: Error?) in
            if isSuccess {
                message = "ä¿å­˜æˆåŠŸ!"
            } else{
                message = "ä¿å­˜å¤±è´¥ï¼š\(error!.localizedDescription)"
            }
            
            DispatchQueue.main.async {
                //å¼¹å‡ºæç¤ºæ¡†
                let alertController = UIAlertController(title: message, message: nil,
                                                        preferredStyle: .alert)
                let cancelAction = UIAlertAction(title: "ç¡®å®š", style: .cancel, handler: { alert in
                    self.reviewRecord(outputURL: outputFileURL)
                })
                alertController.addAction(cancelAction)
                
                self.present(alertController, animated: true, completion: nil)
            }
        })
    }
    
    
    
    func detectedFace(request: VNRequest, error: Error?) {
        // 1
        guard
            let results = request.results as? [VNFaceObservation],
            let result = results.first
        else {
            // 2
            //          faceView.clear()
            print("ğŸ”´ = " + #function)
            return
        }
        
        //      if faceViewHidden {
        //        updateLaserView(for: result)
        //      } else {
        //        updateFaceView(for: result)
        //      }
    }
    
    
}

//äººè„¸
extension TestViedo: AVCaptureVideoDataOutputSampleBufferDelegate {
    // äººè„¸è¯†åˆ«
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        // 1
        guard let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            print("ğŸ”´ = " + #function)
            //æœªè¯†åˆ«åˆ°
            return
        }
        print("ğŸŸ£")
        
        // 2
        let detectFaceRequest = VNDetectFaceLandmarksRequest(completionHandler: detectedFace)
        
        // 3
        do {
            try sequenceHandler.perform(
                [detectFaceRequest],
                on: imageBuffer,
                orientation: .leftMirrored)
            print("ğŸŸ¢ = " + #function)
            
        } catch {
            print("ğŸ”´ = " + #function)
            
            print(error.localizedDescription)
        }
    }
    
    func setupFace() {
        
        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                   for: .video,
                                                   position: .back) else {
            fatalError("No front video camera available")
        }
        do {
            let cameraInput = try AVCaptureDeviceInput(device: camera)
            session.addInput(cameraInput)
            
        } catch {
            fatalError(error.localizedDescription)
        }
        
        // Create the video data output
        //        let videoOutput = AVCaptureVideoDataOutput()
        videoOutputFace.setSampleBufferDelegate(self, queue: dataOutputQueue)
        videoOutputFace.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]
        // åŠ å…¥è„¸éƒ¨è¯†åˆ«
        //            // Add the video output to the capture session
        session.addOutput(videoOutputFace)
        
        let videoConnection = videoOutputFace.connection(with: .video)
        videoConnection?.videoOrientation = .portrait
        //        session.startRunning()
    }
}

// MARK: UI
extension TestViedo {
    func setUpPreview() {
        do{
            
            let videoInput1 = try! AVCaptureDeviceInput(device: self.videoDevice!)
            self.captureSession.addInput(videoInput1)
            //            let audioInput1 = try! AVCaptureDeviceInput(device: self.audioDevice!)
            //            self.captureSession.addInput(audioInput1);
            
            //            videoOutputFace
            //æ·»åŠ è§†é¢‘æ•è·è¾“å‡º
            self.captureSession.addOutput(self.fileOutput)
            printLog(captureSession)

            //ä½¿ç”¨AVCaptureVideoPreviewLayerå¯ä»¥å°†æ‘„åƒå¤´çš„æ‹æ‘„çš„å®æ—¶ç”»é¢æ˜¾ç¤ºåœ¨ViewControllerä¸Š
            let videoLayer = AVCaptureVideoPreviewLayer(session: self.captureSession)
            //é¢„è§ˆçª—å£æ˜¯æ­£æ–¹å½¢ï¼Œåœ¨å±å¹•å±…ä¸­ï¼ˆæ˜¾ç¤ºçš„ä¹Ÿæ˜¯æ‘„åƒå¤´æ‹æ‘„çš„ä¸­å¿ƒåŒºåŸŸï¼‰
            videoLayer.frame = CGRect(x:self.view.bounds.height/4,
                                      y:self.view.bounds.height/4,
                                      width: self.view.bounds.width/2,
                                      height: self.view.bounds.height/2)
            videoLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill
            self.view.layer.addSublayer(videoLayer)
            
            //å¯åŠ¨sessionä¼šè¯
            setUpButton()
//            self.captureSession.startRunning()
            
        } catch {
            
            // ã‚¨ãƒ©ãƒ¼å‡¦ç†
            printErrorLog(2)
        }
    }
    
    func setUpButton() {
        recordButton=UIButton(frame:CGRect(x:0,y:0,width:120,height:50))
        recordButton.backgroundColor = UIColor.gray
        recordButton.layer.masksToBounds = true
        recordButton.setTitle("éŒ²ç”»é–‹å§‹", for:UIControl.State.normal)
        recordButton.layer.cornerRadius = 20.0
        recordButton.layer.position = CGPoint(x: self.view.bounds.width/2, y:self.view.bounds.height-50)
        recordButton.addTarget(self, action: #selector(onClickRecordButton(sender:)), for: .touchUpInside)
        self.view.addSubview(recordButton)
        printLog(recordButton)
        
    }
    
}

//å…±é€š
extension TestViedo {
    //å½•åƒå›çœ‹
    func reviewRecord(outputURL: URL) {
        //å®šä¹‰ä¸€ä¸ªè§†é¢‘æ’­æ”¾å™¨ï¼Œé€šè¿‡æœ¬åœ°æ–‡ä»¶è·¯å¾„åˆå§‹åŒ–
        let player = AVPlayer(url: outputURL)
        let playerViewController = AVPlayerViewController()
        playerViewController.player = player
        self.present(playerViewController, animated: true) {
            playerViewController.player!.play()
        }
    }
}

extension TestViedo {
    func otherVC() {
        //        let a = FaceDetectionViewController.init(nibName: "main", bundle: Bundle.init(identifier: "FaceDetectionViewController"))
        let sb = UIStoryboard.init(name: "Main", bundle: nil)
        var a = sb.instantiateViewController(withIdentifier: "FaceDetectionViewController") as! FaceDetectionViewController
        a.configureCaptureSession()
        //        a.session.startRunning()
        
    }
}

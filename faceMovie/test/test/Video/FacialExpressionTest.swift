//
//  TestViedo.swift
//  test
//
//  Created by cyanboo on 2022/7/13.
//

import Foundation
import UIKit
import AVFoundation
import Photos
import Vision
import AVKit

class FacialExpressionTest: UIViewController {
    
    ///å½•åƒ
    // å½•åƒæŒ‰é’®
    @IBOutlet var recordButton: UIButton!
    // æ­£åœ¨å½•éŸ³
    var isRecording = false
    //æœ€å¤§å…è®¸çš„å½•åˆ¶æ—¶é—´ï¼ˆç§’ï¼‰
    let videoTimeCount = TimeInterval(15)
    let totalSeconds: Float64 = Float64( 15 * 2.00 + 3.0)
    //æ¯ç§’å¸§æ•°
    var framesPerSecond:Int32 = 30
    //è§†é¢‘æ•è·ä¼šè¯ã€‚å®ƒæ˜¯inputå’Œoutputçš„æ¡¥æ¢ã€‚å®ƒåè°ƒç€intputåˆ°outputçš„æ•°æ®ä¼ è¾“
    let captureSession = AVCaptureSession()
    //è§†é¢‘è¾“å…¥è®¾å¤‡
    let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera, for: AVMediaType.video, position: .front)
    //    let movieDevice = AVCaptureDevice.default(for: .audio
    //éŸ³é¢‘è¾“å…¥è®¾å¤‡
    let audioDevice = AVCaptureDevice.default(for: AVMediaType.audio)
    //å°†æ•è·åˆ°çš„è§†é¢‘è¾“å‡ºåˆ°æ–‡ä»¶
    let fileOutput = AVCaptureMovieFileOutput()
    //ä¿å­˜æ‰€æœ‰çš„å½•åƒç‰‡æ®µæ•°ç»„
    var videoAssets = [AVAsset]()
    //ä¿å­˜æ‰€æœ‰çš„å½•åƒç‰‡æ®µurlæ•°ç»„
    var assetURLs = [String]()
    //å•ç‹¬å½•åƒç‰‡æ®µçš„indexç´¢å¼•
    var appendix: Int32 = 1
    
    @IBOutlet var testImage: UIImageView!
    
    ///è„¸éƒ¨è¯†åˆ«
    let faceSession = AVCaptureSession()
    let dataOutputQueue = DispatchQueue(
        label: "video data queue",
        qos: .userInitiated,
        attributes: [],
        autoreleaseFrequency: .workItem)
    var faceViewHidden = false
    var sequenceHandler = VNSequenceRequestHandler()
    var videoOutputFace = AVCaptureVideoDataOutput()
    var videoLayer = AVCaptureVideoPreviewLayer()
    var faceCount = 0{
        willSet{
            printLog(newValue)
            DispatchQueue.main.sync { [weak self] in
                if newValue <= 5 {
                    self!.recordButton.backgroundColor = whiteColorE4E4E4.uicolor()
                    self!.recordButton.isUserInteractionEnabled = false
                    self!.testImage.image = UIImage.init(named: "QRCode_gray_ScanBox")
                } else {
                    self!.recordButton.backgroundColor = blueColor365CDC.uicolor()
                    self!.recordButton.isUserInteractionEnabled = true
                    self!.testImage.image = UIImage.init(named: "QRCode_ScanBox")
                }
            }
            
            
        }
        didSet{
            
        }
    }
    
    var faceType: FaceType = .videoFirst
    var videoTimeFirst3S: Timer?
    var videoTimeSecond15S: Timer?
    var videoTimeThird15S: Timer?
    var videoTimeCountDown: Timer?
    var countDownNumber: Int = 0
    
    var noteLabelText = "çœŸé¡”ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚"
    private let blueColor365CDC = "365CDC"
    private let grayColor999999 = "999999"
    public let whiteColorE4E4E4 = "E4E4E4"
    private let makeVedioButtonTitle = "æ’®å½±ã™ã‚‹"
    private let shootingButtonTitle = "æ’®å½±ä¸­"
    
    
    @IBOutlet var countDownNumberLabel: UILabel!
    
    @IBOutlet var videoLayerImageView: UIImageView!
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        faceType = .videoFirst
        // è®¾ç½®é¢„è§ˆç”»é¢
        setUpPreview()
        setupFace()
        
        
    }
    
    override func viewWillAppear(_ animated: Bool) {
        super.viewWillAppear(animated)
    }
}

// MARK: å½•åƒ delegate
extension FacialExpressionTest: AVCaptureFileOutputRecordingDelegate {
    //å½•åƒå¼€å§‹çš„ä»£ç†æ–¹æ³•
    func fileOutput(_ output: AVCaptureFileOutput,
                    didStartRecordingTo fileURL: URL,
                    from connections: [AVCaptureConnection]) {
        printLog("å½•åƒå¯åŠ¨çš„ä»£ç†æ–¹æ³•")
    }
    
    func fileOutput(_ output:AVCaptureFileOutput, didFinishRecordingTo outputFileURL:URL, from connections: [AVCaptureConnection], error:Error?) {
        printLog(output)
        
        let asset = AVURLAsset(url: outputFileURL, options: nil)
        videoAssets.append(asset)
        assetURLs.append(outputFileURL.path)
        videoTimeCountDown?.invalidate()
        
        printGreenLog("è§†é¢‘æ‹æ‘„ç»“æŸæ­£åœ¨å¤„ç†æ–‡ä»¶")
        //        changeFaceTypeStatus()
        //        if faceType == .finshed{
        //            saveVideoToDocument(outputFileURL: outputFileURL)
        //        }
        saveVideoToDocument(outputFileURL: outputFileURL)
        faceType = .finshed
        
        return
        if faceType == .videoFirst {
            faceType = .videoSecond
            printGreenLog("è§†é¢‘æ‹æ‘„ç»“æŸæ­£åœ¨å¤„ç†æ–‡ä»¶--- å˜æ›´ factype = second ğŸ”µ")
            
            videoSecond15S()
            startRecord()
            
        } else if faceType == .videoSecond {
            faceType = .videoThird
            printGreenLog("è§†é¢‘æ‹æ‘„ç»“æŸæ­£åœ¨å¤„ç†æ–‡ä»¶--- å˜æ›´ factype = third ğŸ”µ")
            
            videoThird15S()
            startRecord()
        } else if faceType == .videoThird {
            faceType = .finshed
            printGreenLog("è§†é¢‘æ‹æ‘„ç»“æŸæ­£åœ¨å¤„ç†æ–‡ä»¶--- å˜æ›´ factype = finshed ğŸ”µ")
            
            saveVideoToDocument(outputFileURL: outputFileURL)
            countDownNumberLabel.isHidden = true
            
        }else if faceType == .finshed {
        } else {
            printErrorLog("face type is null")
            
        }
        //        saveVideoToDocument(outputFileURL: outputFileURL)
    }
    
    
    
}

// MARK: äººè„¸ delegate
extension FacialExpressionTest: AVCaptureVideoDataOutputSampleBufferDelegate {
    // äººè„¸è¯†åˆ«è¿‡ç¨‹
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            //æœªè¯†åˆ«åˆ° - å¤±è´¥
            printErrorLog(sampleBuffer)
            return
        }
        print("ğŸŸ£")
        let detectFaceRequest = VNDetectFaceLandmarksRequest(completionHandler: detectedFace)
        do {
            try sequenceHandler.perform(
                [detectFaceRequest],
                on: imageBuffer,
                orientation: .leftMirrored)
            printLog(#function)
            
        } catch {
            printErrorLog(error.localizedDescription)
            return
        }
    }
    
    //  åˆå§‹åŒ–äººè„¸è¯†åˆ«åˆ¤æ–­
    func setupFace() {
        
        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                   for: .video,
                                                   position: .front) else {
            return printErrorLog("No back video camera available")
        }
        do {
            let cameraFaceInput = try AVCaptureDeviceInput(device: camera)
            faceSession.addInput(cameraFaceInput)
            
        } catch {
            printErrorLog(error.localizedDescription)
            return
        }
        videoOutputFace.setSampleBufferDelegate(self, queue: dataOutputQueue)
        videoOutputFace.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]
        
        // åŠ å…¥è„¸éƒ¨è¯†åˆ«
        //            // Add the video output to the capture session
        faceSession.addOutput(videoOutputFace)
        
        let videoConnection = videoOutputFace.connection(with: .video)
        videoConnection?.videoOrientation = .portrait
        //        session.startRunning()
        videoLayer = AVCaptureVideoPreviewLayer(session: faceSession)
        videoLayer.videoGravity = .resizeAspectFill
        videoLayer.bounds = videoLayerImageView.bounds
        videoLayer.frame = CGRect(x:0,
                                  y:0,
                                  width: videoLayerImageView.bounds.width,
                                  height: videoLayerImageView.bounds.height)
        
        videoLayerImageView.layer.insertSublayer(videoLayer, at: 0)
        videoLayerImageView.backgroundColor = .yellow
        faceSession.startRunning()
        
    }
    
    // å¾ªåŒ–åˆ¤æ–­face
    func detectedFace(request: VNRequest, error: Error?) {
        // 1
        guard
            let results = request.results as? [VNFaceObservation],
            let _ = results.first
        else {
            print("ğŸ”´ = " + #function)
            faceCount = 0
            return
        }
        faceCount += 1
    }
    
}

// MARK: UI
extension FacialExpressionTest {
    // åˆå§‹åŒ–é•œå¤´æ‘„å½±
    func setUpPreview() {
        let videoInputForMovie = try! AVCaptureDeviceInput(device: self.videoDevice!)
        //            videoInput1.device.position = .front
        self.captureSession.sessionPreset = .hd1280x720
        self.captureSession.addInput(videoInputForMovie)
        //            let audioInput1 = try! AVCaptureDeviceInput(device: self.audioDevice!)
        //            self.captureSession.addInput(audioInput1);
        
        let connection = fileOutput.connection(with: .video)
        if ((connection?.isVideoStabilizationSupported) != nil) {
            connection?.preferredVideoStabilizationMode = .auto
        }
        if fileOutput.availableVideoCodecTypes.contains(.h264) {
            fileOutput.setOutputSettings(["AVVideoCodecKey": AVVideoCodecType.h264], for: connection!)
        }
        //        if let connection = fileOutput.connection(with: .video) {
        //          // è®¾ç½®æ˜¯å¦é•œåƒï¼Œé»˜è®¤æ˜¯ false
        //          connection.isVideoMirrored = true
        //        }
        //æ·»åŠ è§†é¢‘æ•è·è¾“å‡º
        self.captureSession.addOutput(self.fileOutput)
        printLog(captureSession)
        
        //ä½¿ç”¨AVCaptureVideoPreviewLayerå¯ä»¥å°†æ‘„åƒå¤´çš„æ‹æ‘„çš„å®æ—¶ç”»é¢æ˜¾ç¤ºåœ¨ViewControllerä¸Š
        let videoLayer = AVCaptureVideoPreviewLayer(session: self.captureSession)
        videoLayer.bounds = videoLayerImageView.bounds
        videoLayer.frame = CGRect(x:0,
                                  y:0,
                                  width: videoLayerImageView.bounds.width,
                                  height: videoLayerImageView.bounds.height)
        
        videoLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill
        //            self.view.layer.addSublayer(videoLayer)
        videoLayerImageView.layer.insertSublayer(videoLayer, at: 0)
        //        videoLayerImageView.center = self.view.center
        setUpButton()
        
        
        //  è®¾ç½®è§†é¢‘æ¸…æ™°åº¦
        //                  captureSession.sessionPreset = AVCaptureSession.Preset.vga640x480
    }
    
    // æ‘„å½±æŒ‰é’®
    func setUpButton() {
        recordButton.backgroundColor = blueColor365CDC.uicolor()
        
        recordButton.layer.masksToBounds = true
        recordButton.setTitle(makeVedioButtonTitle, for:UIControl.State.normal)
        recordButton.setTitleColor(.white, for: .normal)
        
        recordButton.setTitle(makeVedioButtonTitle, for:UIControl.State.disabled)
        recordButton.setTitleColor(grayColor999999.uicolor(), for: .disabled)
        
        recordButton.setTitle(makeVedioButtonTitle, for:UIControl.State.highlighted)
        recordButton.setTitleColor(.white, for: .highlighted)
        
        recordButton.layer.cornerRadius = 15.0
        recordButton.layer.position = CGPoint(x: self.view.bounds.width/2, y:self.view.bounds.height-50)
        recordButton.addTarget(self, action: #selector(onClickRecordButton(sender:)), for: .touchUpInside)
        
    }
    
    // å°æ®µè§†é¢‘å¼€å§‹å½• UIå¤„ç†
    func saveVideoByfile() {
        saveShortVideoAndConfiguration()
        isRecording=true
        recordButton.setTitle(shootingButtonTitle, for: .normal)
        recordButton.setTitleColor(grayColor999999.uicolor(), for: .normal)
        
    }
    
    // å°æ®µè§†é¢‘å½•å®Œ UIå¤„ç†
    func finshVideo() {
        isRecording=false
        if faceType == .finshed{
            recordButton.setTitle(makeVedioButtonTitle, for: .normal)
            recordButton.setTitleColor(.white, for: .normal)
            recordButton.backgroundColor = blueColor365CDC.uicolor()
        }
    }
}

//MARK: äº¤äº’
extension FacialExpressionTest {
    // MARK: æ‘„å½±æŒ‰é’®ç‚¹å‡»äº‹ä»¶
    @objc func onClickRecordButton(sender: UIButton) {
        faceOver()
        startRecord()
        videoFirst15S()
        //        faceSession.stopRunning()
        //        printLog(isRecording)
        
        if !isRecording {
            //            self.captureSession.startRunning()
            //            printLog(isRecording)
            //            saveVideoByfile()
            //            testImage.isHidden = true
            //            self.captureSession.startRunning()
            startRecord()
            
        } else {
            // éŒ²ç”»çµ‚äº†
            //            fileOutput.stopRecording()
            //            finshVideo()
            //                        finshRecord()
        }
    }
    
    func faceOver() {
        faceSession.stopRunning()
        printLog(isRecording)
    }
    
    func startRecord() {
        printGreenLog("æ‘„å½±å¼€å§‹ ğŸŸ¡")
        if faceType == .finshed {
            return
        }
        self.captureSession.startRunning()
        printLog(isRecording)
        saveVideoByfile()
        testImage.isHidden = true
        countDown()
    }
    
    func finshRecord() {
        printGreenLog("æ‘„å½±ç»ˆäº† ğŸŸ£")
        if faceType == .finshed {
            countDownNumberLabel.isHidden = true
        }
        // éŒ²ç”»çµ‚äº†
        fileOutput.stopRecording()
        finshVideo()
    }
}

//MARK: ä¸šåŠ¡å¤„ç†
extension FacialExpressionTest {
    //å½•åƒå›çœ‹ æ–¹ä¾¿å½•åƒæŸ¥çœ‹,æäº¤æ—¶åˆ é™¤
    func reviewRecord(outputURL: URL) {
        //å®šä¹‰ä¸€ä¸ªè§†é¢‘æ’­æ”¾å™¨ï¼Œé€šè¿‡æœ¬åœ°æ–‡ä»¶è·¯å¾„åˆå§‹åŒ–
        let player = AVPlayer(url: outputURL)
        let playerViewController = AVPlayerViewController()
        playerViewController.player = player
        self.present(playerViewController, animated: true) {
            playerViewController.player!.play()
        }
    }
    
    //ä¿å­˜åˆ°æŒ‡å®šç›®å½• æ ¼å¼video mp4
    func saveShortVideoAndConfiguration() {
        DispatchQueue.main.async { [weak self] in
            self?.recordButton.backgroundColor = self?.whiteColorE4E4E4.uicolor()
            
        }
        // éŒ²ç”»é–‹å§‹
        let paths = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true)
        let documentsDirectory = paths[0] as String
        let filePath : String? = "\(documentsDirectory)/\(getFaceTypeAddress()).mp4"
        let fileURL :NSURL = NSURL(fileURLWithPath: filePath!)
        
        fileOutput.startRecording(to: fileURL as URL, recordingDelegate:self)
    }
    
    // å°†å½•åˆ¶å¥½çš„å½•åƒä¿å­˜åˆ°ç…§ç‰‡åº“ä¸­
    func XTestSaveVideoToPHPhotoLibrary(_ outputFileURL: URL) {
        PHPhotoLibrary.shared().performChanges({
            PHAssetChangeRequest.creationRequestForAssetFromVideo(atFileURL: outputFileURL)
            //å°†è§†é¢‘è½¬æˆData
            //            let data = NSData(contentsOf: outputFileURL)
            //            print(data)
            printLog(outputFileURL)
            
        }, completionHandler: { (isSuccess: Bool, error: Error?) in
            
        })
    }
    
    func saveVideoToDocument(outputFileURL: URL) {
        mergeVideos()
        //        getVedioByself()
    }
    
    //åˆå¹¶è§†é¢‘ç‰‡æ®µ
    func mergeVideos() {
        let duration = totalSeconds
        
        let composition = AVMutableComposition()
        //åˆå¹¶è§†é¢‘
        let firstTrack = composition.addMutableTrack(
            withMediaType: AVMediaType.video, preferredTrackID: CMPersistentTrackID())
        //        let audioTrack = composition.addMutableTrack(
        //            withMediaType: AVMediaType.audio, preferredTrackID: CMPersistentTrackID())
        
        var insertTime: CMTime = CMTime.zero
        for asset in videoAssets {
            do {
                try firstTrack?.insertTimeRange(
                    CMTimeRangeMake(start: CMTime.zero, duration: asset.duration),
                    of: asset.tracks(withMediaType: AVMediaType.video)[0] ,
                    at: insertTime)
            } catch _ {
                printErrorLog("åˆå¹¶å¤±è´¥")
            }
            //            do {
            //                try audioTrack?.insertTimeRange(
            //                    CMTimeRangeMake(start: CMTime.zero, duration: asset.duration),
            //                    of: asset.tracks(withMediaType: AVMediaType.audio)[0] ,
            //                    at: insertTime)
            //            } catch _ {
            //            }
            
            insertTime = CMTimeAdd(insertTime, asset.duration)
        }
        
        //æ—‹è½¬è§†é¢‘å›¾åƒï¼Œé˜²æ­¢90åº¦é¢ å€’
        firstTrack?.preferredTransform = CGAffineTransform(rotationAngle: CGFloat.pi/2)
        
        //å®šä¹‰æœ€ç»ˆç”Ÿæˆçš„è§†é¢‘å°ºå¯¸ï¼ˆçŸ©å½¢çš„ï¼‰
        printLog("è§†é¢‘åŸå§‹å°ºå¯¸ï¼š")
        printLog(firstTrack!.naturalSize)
        let renderSize = CGSize(width: firstTrack!.naturalSize.width,
                                height:firstTrack!.naturalSize.height)
        //        let renderSize = CGSize(width: 1280,
        //                                height: 960)
        
        printLog("æœ€ç»ˆæ¸²æŸ“å°ºå¯¸ï¼š")
        printLog(renderSize)
        //é€šè¿‡AVMutableVideoCompositionå®ç°è§†é¢‘çš„è£å‰ª(çŸ©å½¢ï¼Œæˆªå–æ­£ä¸­å¿ƒåŒºåŸŸè§†é¢‘)
        let videoComposition = AVMutableVideoComposition()
        videoComposition.frameDuration = CMTimeMake(value: 1, timescale: framesPerSecond)
        videoComposition.renderSize = renderSize
        
        printLog("æˆªå–æ­£ä¸­å¿ƒåŒºåŸŸè§†é¢‘")
        printLog(videoComposition.renderSize)
        let instruction = AVMutableVideoCompositionInstruction()
        instruction.timeRange = CMTimeRangeMake(
            start: CMTime.zero,
            duration: CMTimeMakeWithSeconds(Float64(duration),
                                            preferredTimescale: framesPerSecond))
        
        //è§†é¢‘ç¼©æ”¾å’Œæ—‹è½¬
        let transformer: AVMutableVideoCompositionLayerInstruction =
        AVMutableVideoCompositionLayerInstruction(assetTrack: firstTrack!)
        
        printLog(firstTrack?.naturalSize.width as Any)
        //                let t1 = CGAffineTransform(a: -1, b: 0, c: 0, d: 1, tx: (firstTrack!.naturalSize.width ) * 0.20, ty: -(firstTrack!.naturalSize.width - firstTrack!.naturalSize.height)/2)
        // ä¸ªäººæ„Ÿè§‰ txå’Œty è·Ÿè£å‰ªæœ‰å…³;    aå’Œd æ˜¯æ§åˆ¶é•œé¢ç¿»è½¬
        //        let t1 = CGAffineTransform(a: -1, b: 0, c: 0, d: 1, tx: self.view.frame.size.width * 0.25, ty: self.view.frame.size.height * -0.1)
        
        //        t1.scaledBy(x: 0, y: 10)
        printLog("firstTrack --- ")
        printLog(firstTrack?.nominalFrameRate)
        printLog(firstTrack?.naturalSize)
        
        //        let t1 = CGAffineTransform(scaleX: 0.5, y: 0.5)
        //let t1 = CGAffineTransform(translationX: firstTrack!.naturalSize.height,
        //    y: -(firstTrack!.naturalSize.width - firstTrack!.naturalSize.height)/2)
        
        //        var t1: CGAffineTransform = CGAffineTransform(a: -1, b: 0, c: 0, d: 1,tx: firstTrack!.naturalSize.height, ty: 0)
        //        var t2: CGAffineTransform = t1.rotated(by: CGFloat(M_PI_2)/2)
        //(0 0; 1024 1366)
        let t1 = CGAffineTransform(a: -1, b: 0, c: 0, d: 1, tx: 1024 * 0.2745, ty: self.view.frame.size.height * -0.1 )
        
        let t2 = t1.rotated(by: CGFloat.pi/2)
        let finalTransform: CGAffineTransform = t2
        //        transformer.setTransform(finalTransform.inverted(), at: CMTime.zero)
        
        transformer.setTransform(finalTransform, at: CMTime.zero)
        
        
        instruction.layerInstructions = [transformer]
        videoComposition.instructions = [instruction]
        
        //è·å–åˆå¹¶åçš„è§†é¢‘è·¯å¾„
        let documentsPath = NSSearchPathForDirectoriesInDomains(.documentDirectory,
                                                                .userDomainMask,true)[0]
        let destinationPath = documentsPath + "/mergeVideo-\(arc4random()%1000).mp4"
        printLog("åˆå¹¶åçš„è§†é¢‘è·¯å¾„ï¼š \(destinationPath)")
        
        let videoPath = URL(fileURLWithPath: destinationPath as String)
        let exporter = AVAssetExportSession(asset: composition,
                                            presetName:AVAssetExportPresetHighestQuality)!
        //AVFileTypeQuickTimeMovie
        exporter.outputURL = videoPath
        exporter.outputFileType = AVFileType.mp4
        exporter.videoComposition = videoComposition //è®¾ç½®videoComposition
        exporter.shouldOptimizeForNetworkUse = true
        exporter.timeRange = CMTimeRangeMake(
            start: CMTime.zero,
            duration: CMTimeMakeWithSeconds(Float64(33.0), preferredTimescale: framesPerSecond))
        exporter.exportAsynchronously(completionHandler: {
            switch exporter.status {
            case  .failed:
                if let e = exporter.error {
                    print("export failed \(e)")
                }
            case .cancelled:
                print("export cancelled \(String(describing: exporter.error))")
            default:
                //å°†åˆå¹¶åçš„è§†é¢‘ä¿å­˜åˆ°ç›¸å†Œ
                self.exportDidFinish(session: exporter)
            }
            
        })
    }
    
    //å°†åˆå¹¶åçš„è§†é¢‘ä¿å­˜åˆ°ç›¸å†Œ
    func exportDidFinish(session: AVAssetExportSession) {
        print("è§†é¢‘åˆå¹¶å®Œæˆï¼")
        let outputURL = session.outputURL!
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 3) {
            self.delayExecution(url: outputURL)
        }
    }
    func delayExecution(url: URL) {
        self.reviewRecord(outputURL: url)
    }
}

